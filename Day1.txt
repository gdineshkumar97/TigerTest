Bigdata Hadoop and Spark Developer   --- Day1
Trainer: Prashant Nair
===========================================================================================================================================================

Agenda
-------
Introduction to Bigdata
Data Processing Types
Typical Bigdata Processing phases
Introduction to Hadoop
Hadoop Generation
Hadoop Versions
HDFS iN Detail
Lab: HDFS Operations


What is Bigdata?
--------------------
Bigdata is a relative term. Its not a technology. Its not a tool.

Bigdata is all about the ability of the software/framework/architecture to HANDLE the data.
HANDLE refers to 
	a. LOAD
	b. PROCESS
	c. STORE
If any of the above step fails, I can say I am facing a bigdata problem.


OLTP and OLAP
---------------

OLTP - OnLine Transactional Processing
OLAP - OnLine Analytical Processing

OLTP 							OLAP

- Meant to deal with realtime activities		- Meant to deal with historical data.
 and realtime data.

- Mostly we deal with Stream Data			- Mostly we deal with Stored Data (File/DB Table)

- In this you can use Commit and Rollback		- Data Analytics, Intelligence Gathering


Bigdata Processing Types
--------------------------

						Processing Types
							|
			---------------------------------------------------------------------
			|								    |
		Batch Processing							Realtime Processing

	- Meant to deal with Historical Data					- Meant to deal with Streaming data
	- Data will be stored on DISK						- Data is received via streams
 	- Speed of execution depends on DISK					- Speed of Execution depends on Network Speed
	- OLAP based System							- OLTP Based System



Typical Bigdata Project Phases: (Batch Processing)
--------------------------------
The below phases are just a guideline shared by Prashant Nair. You can modify it or use it as it is for your bigdata projects.
There exists 5 phases:

a. Data Acquisition Phase
b. Data Preprocessing Phase
c. Data Transformation Phase
d. Data View Phase
e. Intelligence Extraction Phase



a. Data Acquisition Phase: This phase is all about getting the RAW data from the data sources and storing the same in a persistant storage.

				COLLECT AND STORE


File -------------------------- copyFromLocal --------------------------------HDFS
				put
				Hue

Database---------------------- Apache Sqoop ----------------------------------HDFS


Streams ---------------------- Apache Flume ----------------------------------HDFS
				Apache Kafka




b. Data Pre-processing Phase: This phase is all about preparing your data for the transformation phase. This phase is all about making your data compatible for the next phase.

		Goal:
			a. To unify the set of data in one format
			b. To convert the data from one type to another
			c. To decrypt an encrypted data



e.g. Attendance System

IoT device ---> JSON
Excel Sheet --> Excel -------------------> Unify the data in one format -------> .CSV -------> HDFS
Web Page -----> XML				(UNION)(CSV)


c. Data Transformation Phase: This phase is responsible to perform data transformation (FILTER, GROUP BY, JOIN, UNION etc)




 .CSV ------> X tool -------> Filter -------------> CSV ------------> HDFS


d. Data View Phase: This phase is responsible to make your processed data QUERY-ABLE

e.g. SQL, NoSQL


e. Intelligence Layer Phase: This phase is all about extracting intelligence out of the processed data usinG AI technoque



Expectation from Bigdata Frameworks -> The tool's capability must be directly proportional to its underlying hardware.

As a data engineer 1-3,if senior 1-4

Hadoop
-------

Hadoop's capability is directly proportional to its underlying hardware's capability.

Hadoop is a Java-based software Framework running on top of Linux OS that offers distributed processing engine with a distributed storage facility.



							Hadoop
							   |
				---------------------------------------------------------
				|							|
		Storage part of Hadoop					Processing part of Hadoop
		HDFS - Hadoop Distributed File System			MR - MapReduce



						Hadoop Generations
							|
			----------------------------------------------------------------------
			|				|				     |
		Generation1			Generation2				Generation3	
		hadoop-1.2.1			hadoop-2.9.x				hadoop-3.1.x

		
						


							Hadoop Gen1
							   |
				---------------------------------------------------------
				|							|
		Storage part of Hadoop					Processing part of Hadoop
		HDFS - Hadoop Distributed File System			MR - MapReduce




							Hadoop Gen2 / Gen3
							   |
				-------------------------------------------------------------------------------------------------------------
				|							|						    |
		Storage part of Hadoop					Processing part of Hadoop		           Resource Management Layer
		HDFS - Hadoop Distributed File System			MR - MapReduce				YARN - Yet Another Resource Negotiator




Storage part of Hadoop
-----------------------

HDFS - Hadoop Distributed File System

HDFS Daemons
	a. Namenode (Master): This daemon is responsible to maintain the metadata of teh data that is stored in the HDFS.
				Index information
	b. DataNode(Slave): This daemon is responsible for the actual read and write IO operations

HDFS is an immutable storage (Once created, it cannot be changed/modified)



HDFS Write operation
----------------------

1. Client sends request to the Namenode machine

2. Once the request is received by the NN, the NN will perform the following checks:
	a. Check if the destination folder exists or not. If destination exists, throw error else go to step b.
	b. Check if HDFS has enough space. If disk space available, go to step c else throw error
	c. NN will create logical blocks for the file based on current block size.
		HadoopGen1 --- Default Block Size is 64MB
		HadoopGen2/3 - Default Block Size is 128MB
	d. NN will allocate each block to the respective datanodes such that storage is well balanced
	e. NN will share the metadata to the client machine

3. Once the client receives the metadata, client will initiate a PUSH request for the blocks

4. Client will wait for the ack from each datanote.

5. Once client receives ack from all responsible datanodes, client will send a final ack to the NN confirming tht the data is written successfully in the cluster.




HDFS Read Operation
===================


1. Client will send request to the Namenode (copyToLocal, get, Hue)

2. Once the namenode receives the request, NN will check the following

	a. Check if the file exists, if yes proceed to b else throw error
	b. Check if the associated DNs are alive or not. If available go to c else throw error
	c. Gather relevant metadata and provide the same to the Client machine

3. Once the client receives the metadata, client will initiate the connection between himself and the relevant datanodes and perform a pull operation. Once pull completed, client will give ack to the respective DNs

4. Once all blocks are received, client will stitch the blocks to form a file based on the metadata information.





Lab1: HDFS Commands Introduction

a. Create a Directory in HDFS

Using WebConsole:
	
	hdfs dfs -mkdir 04april2022

b. List the directories in HDFS home location

	hdfs dfs -ls 

c. Upload the data in HDFS

	1. Upload the dataset from your laptop/desktop to Linux Webconsole machine using FTP (04April2022/Day1.txt)
	
	2. Upload the dataset from linux to HDFS
		hdfs dfs -copyFromLocal 04April2022/Day1.txt 04april2022/day1.txt

		hdfs dfs -copyFromLocal 04April2022/Day1.txt 04april2022/day1.txt
		 hdfs dfs -put 04April2022/Day1.txt 04april2022/test1.txt

d. Upload the data in HDFS using Hue

	1. Create destination folder
	2. Click on Upload > Upload Files > Select file 


e. Delete a file from HDFS

	hdfs dfs -rm -rf 04april2022/day2.txt







Demo on HDFS read

hdfs dfs -copyToLocal 04april2022/day2.txt day2.txt

hdfs dfs -get 04april2022/day2.txt day2.txt











































